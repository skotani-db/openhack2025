{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d122cbec-9051-4c42-90b7-6edfccd4dd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1/ LLM Chatbot RAGのデータ準備\n",
    "\n",
    "## Databricks Vector Searchを用いたナレッジベースの構築とインデックス化\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-managed-flow-1.png?raw=true\" style=\"float: right; width: 800px; margin-left: 10px\">\n",
    "\n",
    "このノートブックでは、ドキュメントページを取り込み、Vector Searchインデックスを作成して、チャットボットの回答精度を向上させる方法を学びます。\n",
    "\n",
    "高品質なデータを準備することは、チャットボットの性能向上において非常に重要です。ぜひ、ご自身のデータセットを使用して、次のステップを実装してみてください。\n",
    "\n",
    "幸いにも、Lakehouse AIは、AIおよびLLMプロジェクトを加速する最先端のソリューションを提供しており、大規模なデータ取り込みや準備を簡素化します。\n",
    "\n",
    "この例では、[docs.databricks.com](https://docs.databricks.com) からDatabricksのドキュメントを使用します：\n",
    "- ウェブページをダウンロードする\n",
    "- ページを小さなテキストチャンクに分割する\n",
    "- Databricks Foundationモデルを使用して埋め込みを計算し、それをDelta Tableの一部として保存する\n",
    "- Delta Tableに基づいてVector Searchインデックスを作成する\n",
    "\n",
    "<!-- 使用データの収集（ビュー）。これを削除すると、収集が無効になります。インストール中にトラッカーを無効化することも可能です。詳細はREADMEをご覧ください。 -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=2952210246698934&notebook=%2F02-simple-app%2F01-Data-Preparation-and-Index&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F02-simple-app%2F01-Data-Preparation-and-Index&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea20e82-2bf6-4718-b168-604d9b539235",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ライブラリのインストール"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow lxml==4.9.3 transformers langchain databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ced867-1ed8-4d01-9f01-693aa94f79b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "リソースとカタログの設定"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7da00d-d415-4620-8e2c-c7517822e97c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "本ノートブックで利用するスキーマを作成"
    }
   },
   "outputs": [],
   "source": [
    "# 本ノートブックで利用するスキーマを作成\n",
    "schema_name = f\"05_vector_search_index_for_{user_name}\"\n",
    "print(f\"schema_name: `{schema_name}`\")\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(f\"USE {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4b8f45-cb2a-49f2-bd78-ec55109bd3ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "init"
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57a395c-c678-451a-86e0-bcd242e63f47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "スキーマの確認"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select current_schema();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ad1b49-02ab-4aa0-8ea2-a6c0eac04e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricksドキュメントのサイトマップとページの抽出\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-1.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "**※ openhackのコンテンツ用に説明を書き換える。**  \n",
    "**※ openhackでは、製品のマニュアル(html)を取り込み予定。**\n",
    "\n",
    "まず、Delta Lakeテーブルとして生データセットを作成します。\n",
    "\n",
    "このデモでは、`docs.databricks.com` からいくつかのドキュメントページを直接ダウンロードし、HTMLコンテンツを保存します。\n",
    "\n",
    "主な手順は以下の通りです：\n",
    "\n",
    "- `sitemap.xml` ファイルからページのURLを抽出する簡単なスクリプトを実行\n",
    "- ウェブページをダウンロード\n",
    "- BeautifulSoupを使用してArticleBodyを抽出\n",
    "- HTML結果をDelta Lakeテーブルに保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b271e9cf-c682-4098-a9da-a1d6c9263cac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ドキュメント(html)の読み込み"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# def load_html_files_to_table(input_path: str, table_name: str, mode: str = 'append'):\n",
    "def load_html_files_to_table():\n",
    "    input_path = \"/Volumes/trainer_catalog/default/src_data/openhack-llm/*.html\"\n",
    "    table_name = \"raw_documentation\"\n",
    "    mode = \"append\"\n",
    "    # HTMLファイルを読み込み (wholetext)\n",
    "    df = (\n",
    "        spark.read.format(\"text\")\n",
    "        .option(\"wholetext\", True)\n",
    "        .load(input_path)\n",
    "        .withColumn(\"filename\", F.col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"filename\", F.regexp_replace(F.col(\"filename\"), r'^.*/', ''))\n",
    "        .withColumnRenamed(\"filename\", \"url\")\n",
    "        .withColumnRenamed(\"value\", \"text\")\n",
    "        .select(\"url\", \"text\")\n",
    "    )\n",
    "\n",
    "    # Deltaテーブルとして書き込み\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(table_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e496c6-2d70-46b1-8f2b-d909da7d8e4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "読み込んだドキュメントの書き込み"
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"raw_documentation\") or spark.table(\"raw_documentation\").isEmpty():\n",
    "    doc_articles = load_html_files_to_table()\n",
    "    #Save them as a raw_documentation table\n",
    "    doc_articles.write.mode('overwrite').saveAsTable(\"raw_documentation\")\n",
    "\n",
    "display(spark.table(\"raw_documentation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21420ed-5516-4c2a-ad61-6f44b94c6e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ドキュメントページを小さなチャンクに分割する\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-2.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "LLMモデルには通常、最大入力コンテキスト長があり、非常に長いテキストでは埋め込みを計算できません。  \n",
    "さらに、コンテキストが長くなるほど、モデルが応答を生成するのに時間がかかります。\n",
    "\n",
    "ドキュメントの適切な準備はモデルの性能向上において重要であり、データセットに応じて以下のような戦略があります：\n",
    "\n",
    "- ドキュメントを小さなチャンク（段落、h2など）に分割する\n",
    "- ドキュメントを固定された長さに切り詰める（トランケーション）\n",
    "- チャンクサイズは、コンテンツの特性やプロンプトでの使用方法によって変わる\n",
    "  - 複数の小さなチャンクをプロンプトに含める場合と、大きなチャンクを1つ送信する場合では結果が異なることがある\n",
    "- 大きなチャンクに分割し、それぞれをモデルで一度に要約しておくことで、リアルタイムの推論を高速化\n",
    "- 複数のエージェントを用いて、大きなドキュメントを並行して評価し、最終的なエージェントが回答を生成する戦略を採用\n",
    "\n",
    "これらの方法を試し、データセットやユースケースに合った分割方法を選択してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203a880a-91ed-4b6f-b7d5-d4298df282c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ドキュメントページを小さなチャンク（h2セクション）に分割\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "<br/>\n",
    "このデモでは、長すぎてモデルへのプロンプトに収められない大きなドキュメント記事を扱います。\n",
    "\n",
    "複数のドキュメントをRAG（Retrieval-Augmented Generation）コンテキストとして使用することは、最大入力サイズを超えてしまうため不可能です。また、最近の研究では、ウィンドウサイズを大きくすることが必ずしも効果的ではないとされており、LLM（大規模言語モデル）はプロンプトの冒頭と末尾に重点を置く傾向があると示唆されています。\n",
    "\n",
    "ここでは、記事をHTMLの`h2`タグごとに分割し、HTMLを削除したうえで、LangChainを使用して各チャンクを500トークン未満に収めます。\n",
    "\n",
    "#### LLMのウィンドウサイズとトークナイザー\n",
    "\n",
    "同じ文章でも、異なるモデルでは異なるトークン数になることがあります。LLMには、与えられた文章のトークン数を数えるための`Tokenizer`が付属しており、通常、単語数よりも多くのトークンが生成されます（詳細は[Hugging Faceのドキュメント](https://huggingface.co/docs/transformers/main/tokenizer_summary)や[OpenAIのリポジトリ](https://github.com/openai/tiktoken)をご覧ください）。\n",
    "\n",
    "ここで使用するトークナイザーがモデルと一致していることを確認してください。DatabricksのDBRX InstructモデルはGPT-4と同じトークナイザーを使用します。この例では、`transformers`ライブラリを使用して、DBRX Instructのトークナイザーでトークン数をカウントします。また、ドキュメントのトークンサイズが埋め込みの最大サイズ（1024トークン）を超えないようにします。\n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px;\">\n",
    "  <strong>情報</strong><br/>\n",
    "  以下の手順はデータセットごとに異なります。これは、成功するRAGアシスタントを構築するための重要な部分です。\n",
    "  <br/>作成されたチャンクを手動で確認し、内容が適切で関連性があることを必ず確認してください。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdbdb3c-ceba-46ec-bb00-7032194223f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ドキュメントを小さなチャンクに分割する"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from bs4 import BeautifulSoup\n",
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "max_chunk_size = 500\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "\n",
    "def split_html_with_h3_p(html, min_chunk_size=20, max_chunk_size=500):\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    chunks = []\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for tag in soup.find_all([\"h2\", \"h3\", \"p\"]):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if tag.name == \"h2\":\n",
    "            # h2が新しく来たら、前のチャンクを確定\n",
    "            if current_chunk:\n",
    "                chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "            current_h2 = text\n",
    "            current_h3 = None  # h3をリセット\n",
    "            current_chunk = f\"{current_h2}\\n\"\n",
    "\n",
    "        elif tag.name == \"h3\":\n",
    "            # h3が新しく来たら、前のチャンクを確定\n",
    "            if current_chunk:\n",
    "                chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "            current_h3 = text\n",
    "            current_chunk = f\"{current_h2}\\n{current_h3}\\n\" if current_h2 else f\"{current_h3}\\n\"\n",
    "\n",
    "        elif tag.name == \"p\":\n",
    "            # pは現在のh2またはh3に追加\n",
    "            if len(tokenizer.encode(current_chunk + text)) <= max_chunk_size:\n",
    "                current_chunk += text + \"\\n\"\n",
    "            else:\n",
    "                chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "                current_chunk = text + \"\\n\"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "\n",
    "    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "\n",
    "# DatabricksのテーブルからHTMLデータを取得\n",
    "html = spark.table(\"raw_documentation\").limit(1).collect()[0]['text']\n",
    "split_chunks = split_html_with_h3_p(html)\n",
    "print(split_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32163cc-591d-4f00-ac55-0434fb3a48aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### チャンクの作成とDeltaテーブルへの保存\n",
    "\n",
    "最後のステップは、UDF（ユーザー定義関数）をすべてのドキュメントテキストに適用し、それらを`databricks_documentation`テーブルに保存することです。\n",
    "\n",
    "*この部分は通常、本番環境レベルのジョブとして設定され、新しいドキュメントページが更新され次第、実行される形になります。<br/>Delta Live Tableパイプラインとして設定し、更新を逐次的に処理することも可能です。*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3e0010-896f-4fed-845a-cd19de43c914",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the final documentation table containing chunks"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS product_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f23007-86ea-46c5-b632-c00d5ec4adbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a user-defined function (UDF) to chunk all our documents with spark\n",
    "import pandas as pd\n",
    "\n",
    "@F.pandas_udf(\"array<string>\")\n",
    "def parse_and_split(docs: pd.Series) -> pd.Series:\n",
    "    return docs.apply(split_html_with_h3_p)\n",
    "    # return docs.apply(split_html_on_h2)\n",
    "    \n",
    "(spark.table(\"raw_documentation\")\n",
    "      .filter('text is not null')\n",
    "      .withColumn('content', F.explode(parse_and_split('text')))\n",
    "      .drop(\"text\")\n",
    "      .write.mode('overwrite').saveAsTable(\"product_documentation\"))\n",
    "\n",
    "display(spark.table(\"product_documentation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430bf5e3-22ff-4893-af74-327948bbe943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ベクター検索インデックスの要件\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-managed-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "Databricksでは、以下の複数の種類のベクター検索インデックスを提供しています：\n",
    "\n",
    "- **マネージド埋め込み (Managed embeddings)**  \n",
    "  テキストカラムとエンドポイント名を指定すると、DatabricksがDeltaテーブルとインデックスを同期します。  \n",
    "  **（このデモではこれを使用します）**\n",
    "\n",
    "- **セルフマネージド埋め込み (Self Managed embeddings)**  \n",
    "  埋め込みを自分で計算し、それをDeltaテーブルのフィールドとして保存します。その後、Databricksがインデックスを同期します。\n",
    "\n",
    "- **直接インデックス (Direct index)**  \n",
    "  Deltaテーブルを使用せずにインデックスを利用・更新したい場合に使用します。\n",
    "\n",
    "このデモでは、**マネージド埋め込み**インデックスの設定方法を紹介します。  \n",
    "（セルフマネージド埋め込みは上級デモでカバーされています。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cca752-5eed-4ff4-a313-35d84606647d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks GTE Embeddings Foundation Model エンドポイントの紹介\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-4.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Databricksが提供するファウンデーションモデルは、すぐに利用できる形で提供されています。\n",
    "\n",
    "Databricksでは、埋め込みを計算したりモデルを評価するために、以下のエンドポイントタイプをサポートしています：\n",
    "- **ファウンデーションモデルエンドポイント**（例: DBRX, MPT, GTE）  \n",
    "  Databricksが提供する標準エンドポイント。  \n",
    "  **このデモではこれを使用します。**\n",
    "- **外部エンドポイント**  \n",
    "  外部モデルへのゲートウェイとして機能（例: Azure OpenAI）\n",
    "- **カスタムモデル**  \n",
    "  Databricksモデルサービスにホストされたファインチューニング済みモデル\n",
    "\n",
    "[Model Serving Endpointページ](/ml/endpoints)を開いて、ファウンデーションモデルを探索し試してみてください。\n",
    "\n",
    "このデモでは、埋め込み用にファウンデーションモデルの`GTE`を、チャット用に`DBRX`を使用します。<br/><br/>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-foundation-models.png?raw=true\" width=\"600px\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945a8e7e-8494-4754-b03e-f319b3e44e75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "エンベディングの例"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "\n",
    "# エンベディングモデルをtext-embedding-ada-002に変更\n",
    "\n",
    "# Databricks のデプロイメントクライアントを取得\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# OpenAI の text-embedding-ada-002 を使用\n",
    "response = deploy_client.predict(\n",
    "    endpoint=\"openhack-text-embedding-ada-002\",\n",
    "    inputs={\"input\": [\"What is Apache Spark?\"]}\n",
    ")\n",
    "\n",
    "# 結果を取得\n",
    "embeddings = [e[\"embedding\"] for e in response.data]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617cad67-0be0-4da4-a914-ee3e33887025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Managed EmbeddingsとGTEを用いたベクター検索インデックスの作成\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-3.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Managed Embeddingsを使用することで、Databricksが埋め込みを自動的に計算してくれます。これにより、Databricksの利用を簡単に始められるモードとなっています。\n",
    "\n",
    "ベクター検索インデックスは、埋め込みを提供する**ベクター検索エンドポイント**を使用します。（これをベクター検索APIエンドポイントと考えることができます。）\n",
    "\n",
    "複数のインデックスが同じエンドポイントを利用することも可能です。\n",
    "\n",
    "では、インデックスを作成してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdb79f1-63ec-4ef0-80a7-78869cbc7e20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")\n",
    "\n",
    "# 10分～15分ほどかかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e4d431-9985-46a7-bd75-58b0b1495b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. 現在のノートブックの左型タブにある`Workspace (Ctrl + Alt + E)`を選択し、現在のディレクトリ（`contents`)を表示\n",
    "1. ケバブメニュー（`︙`）を選択し、`作成` -> `Genieスペース`を選択 *1\n",
    "1. Genieスペースの作成画面にて下記セルの出力結果を設定して`Save`を選択\n",
    "1. チャットウィンドウにて、`データセットに含まれるテーブルについて説明して`という質問の回答が来ることを確認\n",
    "\n",
    "*1 Genie スペースを作成できない場合には、下記の手順を実施して Genie スペースの有効化が必要です\n",
    "\n",
    "1. Databricks Workspace にて右上のユーザーアイコンを選択後、`Previews`を選択\n",
    "1. `Genie`の有効化に関するトグルを`On`に設定\n",
    "1. ページをリロード後、Databricks Workspace の左メニューにて Genie が追加されたことを確認 \n",
    "\n",
    "参考リンク\n",
    "\n",
    "- [AI/BI Genie スペースとは](https://learn.microsoft.com/ja-jp/azure/databricks/genie/)\n",
    "- [Use trusted assets in AI/BI Genie spaces](https://learn.microsoft.com/ja-jp/azure/databricks/genie/trusted-assets)\n",
    "- [効果的な Genie スペースをキュレーションする](https://learn.microsoft.com/ja-jp/azure/databricks/genie/best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34b3960-c46a-4175-a6a1-abba64e3da30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\" style=\"float: right\">\n",
    "\n",
    "ベクター検索エンドポイントは[Vector Search Endpoints UI](#/setting/clusters/vector-search)で確認できます。エンドポイント名をクリックすると、そのエンドポイントで提供されるすべてのインデックスを表示できます。\n",
    "\n",
    "### ベクター検索インデックスの作成\n",
    "\n",
    "ここからは、Databricksにインデックスを作成するよう依頼するだけです。\n",
    "\n",
    "Managed Embedding Indexの場合、必要なのはテキストカラムと使用する埋め込みファウンデーションモデル（`GTE`）を指定するだけです。  \n",
    "Databricksが埋め込みの計算を自動的に行います。\n",
    "\n",
    "これはAPIを使用して行うことも、Unity Catalog Explorerメニュー内で数回のクリックで行うこともできます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8719a0f-b218-4c83-b506-3be2b334fca3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the managed vector search using our endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# Openhack ではGUIで実施する\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.product_documentation\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.product_documentation_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  try:\n",
    "    vsc.create_delta_sync_index(\n",
    "      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "      index_name=vs_index_fullname,\n",
    "      source_table_name=source_table_fullname,\n",
    "      pipeline_type=\"TRIGGERED\",\n",
    "      primary_key=\"id\",\n",
    "      embedding_source_column='content', #The column containing our text\n",
    "      # embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "      embedding_model_endpoint_name='openhack-text-embedding-ada-002'\n",
    "    )\n",
    "  except Exception as e:\n",
    "    display_quota_error(e, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "    raise e\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bb5a86-da5e-4326-80e7-665955521287",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "今回取り込んだ html の例(製品FAQのページ)"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Volumes/trainer_catalog/default/src_data/openhack-llm/faq.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "  document_html = f.read()\n",
    "\n",
    "displayHTML(document_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe930b5-537c-429b-9ac6-1a56a639cf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 類似コンテンツの検索\n",
    "\n",
    "これで準備は完了です。DatabricksはDelta Live Table内の新しいエントリを自動的にキャプチャし、同期します。\n",
    "\n",
    "データセットのサイズやモデルの規模に応じて、インデックス作成が始まり、埋め込みをインデックス化するまで数秒かかる場合があります。\n",
    "\n",
    "では、実際に試して類似コンテンツを検索してみましょう。\n",
    "\n",
    "*補足: `similarity_search` ではフィルターパラメータもサポートされています。これはRAGシステムにセキュリティレイヤーを追加するのに便利です。例えば、ユーザーの所属部門に基づいて特定のデータをフィルタリングすることで、機密性の\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f02fcf-d729-432a-8a7a-f0a7dec0073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "question = \"バッテリー交換\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2231505668027387,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "05_vector_search_index",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
