{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d122cbec-9051-4c42-90b7-6edfccd4dd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LLM Chatbot RAGのデータ準備\n",
    "\n",
    "## Databricks Vector Searchを用いたナレッジベースの構築とインデックス化\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-managed-flow-1.png?raw=true\" style=\"float: right; width: 800px; margin-left: 10px\">\n",
    "\n",
    "このノートブックでは、ドキュメントページを取り込み、Vector Searchインデックスを作成して、チャットボットの回答精度を向上させる方法を学びます。\n",
    "\n",
    "高品質なデータを準備することは、チャットボットの性能向上において非常に重要です。ぜひ、ご自身のデータセットを使用して、次のステップを実装してみてください。\n",
    "\n",
    "幸いにも、Lakehouse AIは、AIおよびLLMプロジェクトを加速する最先端のソリューションを提供しており、大規模なデータ取り込みや準備を簡素化します。\n",
    "\n",
    "この例では、Ringo Computer Companyの架空の製品マニュアルのドキュメントを使用します：\n",
    "- ウェブページをダウンロードする\n",
    "- ページを小さなテキストチャンクに分割する\n",
    "- Azure OpenAIのモデルを使用して埋め込みを計算し、それをDelta Tableの一部として保存する\n",
    "- Delta Tableに基づいてVector Searchインデックスを作成する\n",
    "\n",
    "<!-- 使用データの収集（ビュー）。これを削除すると、収集が無効になります。インストール中にトラッカーを無効化することも可能です。詳細はREADMEをご覧ください。 -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=2952210246698934&notebook=%2F02-simple-app%2F01-Data-Preparation-and-Index&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F02-simple-app%2F01-Data-Preparation-and-Index&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad209222-b8a8-4140-8d91-d06e61d2db14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea20e82-2bf6-4718-b168-604d9b539235",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ライブラリのインストール"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow lxml==4.9.3 transformers langchain databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ced867-1ed8-4d01-9f01-693aa94f79b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "リソースとカタログの設定"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7da00d-d415-4620-8e2c-c7517822e97c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "本ノートブックで利用するスキーマを作成"
    }
   },
   "outputs": [],
   "source": [
    "# 本ノートブックで利用するスキーマを作成\n",
    "schema_name = f\"05_vector_search_index_for_{user_name}\"\n",
    "print(f\"schema_name: `{schema_name}`\")\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(f\"USE {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4b8f45-cb2a-49f2-bd78-ec55109bd3ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configファイルの実行"
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ad1b49-02ab-4aa0-8ea2-a6c0eac04e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricksドキュメントのサイトマップとページの抽出\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-1.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "**※ openhackのコンテンツ用に説明を書き換える。**  \n",
    "**※ openhackでは、製品のマニュアル(html)を取り込み予定。**\n",
    "\n",
    "まず、Delta Lakeテーブルとして生データセットを作成します。\n",
    "\n",
    "ここでは、Ringo Computer Companyの架空の製品マニュアルのドキュメントを取り込み、HTMLコンテンツを保存します。\n",
    "\n",
    "主な手順は以下の通りです：\n",
    "\n",
    "- htmlファイルをダウンロード(実利用時はWebページなどをダウンロードします)\n",
    "- BeautifulSoupを使用してArticleBodyを抽出\n",
    "- HTML結果をDelta Lakeテーブルに保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c9bdab-622a-4a04-9d68-16925de60aec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ファイルを読み込む関数を作成"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 指定したパスのHTMLファイルを読み込み、Deltaテーブルとして保存する関数を定義\n",
    "def load_html_files_to_table(input_path: str, table_name: str, mode: str = \"append\"):\n",
    "    \"\"\"\n",
    "    指定したパスのHTMLファイルを読み込み、Deltaテーブルとして保存する関数。\n",
    "\n",
    "    Args:\n",
    "        input_path (str): HTMLファイルのパス（ワイルドカード指定可）\n",
    "        table_name (str): 書き込み先のDeltaテーブル名\n",
    "        mode (str, optional): 書き込みモード（例: 'append', 'overwrite'）。デフォルトは 'append'\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: 読み込んだDataFrame（カラムは 'url' と 'text'）\n",
    "    \"\"\"\n",
    "    # HTMLファイルを全体を1レコードとして読み込む\n",
    "    df = (\n",
    "        spark.read.format(\"text\")\n",
    "        .option(\"wholetext\", True)\n",
    "        .load(input_path)\n",
    "        # ファイルパスからファイル名を抽出して 'filename' カラムに格納\n",
    "        .withColumn(\"filename\", F.col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"filename\", F.regexp_replace(F.col(\"filename\"), r'^.*/', ''))\n",
    "        # カラム名のリネーム: filename -> url, value -> text\n",
    "        .withColumnRenamed(\"filename\", \"url\")\n",
    "        .withColumnRenamed(\"value\", \"text\")\n",
    "        # 必要なカラムのみ選択\n",
    "        .select(\"url\", \"text\")\n",
    "    )\n",
    "\n",
    "    # DataFrameをDeltaフォーマットで指定のテーブルに書き込み\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(table_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e496c6-2d70-46b1-8f2b-d909da7d8e4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ドキュメントをDeltaテーブルへ書き込み"
    }
   },
   "outputs": [],
   "source": [
    "input_path = \"/Volumes/trainer_catalog/default/src_data/openhack-llm/*.html\"\n",
    "table_name = \"raw_documentation\"\n",
    "mode = \"append\"\n",
    "\n",
    "# テーブルが存在しない、または空の場合にデータをロード\n",
    "if not spark.catalog.tableExists(\"raw_documentation\") or spark.table(\"raw_documentation\").isEmpty():\n",
    "    # HTMLファイルからデータを読み込み、DataFrameに格納\n",
    "    doc_articles = load_html_files_to_table(input_path=input_path, table_name=table_name, mode=mode)\n",
    "    # 読み込んだDataFrameをDeltaテーブルとして保存する（上書きモードで保存）\n",
    "    doc_articles.write.mode('overwrite').saveAsTable(\"raw_documentation\")\n",
    "\n",
    "# raw_documentationテーブルの内容をNotebook上に表示する\n",
    "display(spark.table(\"raw_documentation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21420ed-5516-4c2a-ad61-6f44b94c6e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ドキュメントページを小さなチャンクに分割する\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-2.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "LLMモデルには通常、最大入力コンテキスト長があり、非常に長いテキストでは埋め込みを計算できません。  \n",
    "さらに、コンテキストが長くなるほど、モデルが応答を生成するのに時間がかかります。\n",
    "\n",
    "ドキュメントの適切な準備はモデルの性能向上において重要であり、データセットに応じて以下のような戦略があります：\n",
    "\n",
    "- ドキュメントを小さなチャンク（段落、h2など）に分割する\n",
    "- ドキュメントを固定された長さに切り詰める（トランケーション）\n",
    "- チャンクサイズは、コンテンツの特性やプロンプトでの使用方法によって変わる\n",
    "  - 複数の小さなチャンクをプロンプトに含める場合と、大きなチャンクを1つ送信する場合では結果が異なることがある\n",
    "- 大きなチャンクに分割し、それぞれをモデルで一度に要約しておくことで、リアルタイムの推論を高速化\n",
    "- 複数のエージェントを用いて、大きなドキュメントを並行して評価し、最終的なエージェントが回答を生成する戦略を採用\n",
    "\n",
    "これらの方法を試し、データセットやユースケースに合った分割方法を選択してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203a880a-91ed-4b6f-b7d5-d4298df282c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ドキュメントページを小さなチャンク（h2セクション）に分割\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "<br/>\n",
    "このデモでは、長すぎてモデルへのプロンプトに収められない大きなドキュメント記事を扱います。\n",
    "\n",
    "複数のドキュメントをRAG（Retrieval-Augmented Generation）コンテキストとして使用することは、最大入力サイズを超えてしまうため不可能です。また、最近の研究では、ウィンドウサイズを大きくすることが必ずしも効果的ではないとされており、LLM（大規模言語モデル）はプロンプトの冒頭と末尾に重点を置く傾向があると示唆されています。\n",
    "\n",
    "ここでは、記事をHTMLの`h2`,`h3`タグごとに分割し、HTMLを削除したうえで、LangChainを使用して各チャンクを500トークン未満に収めます。\n",
    "\n",
    "#### LLMのウィンドウサイズとトークナイザー\n",
    "\n",
    "同じ文章でも、異なるモデルでは異なるトークン数になることがあります。LLMには、与えられた文章のトークン数を数えるための`Tokenizer`が付属しており、通常、単語数よりも多くのトークンが生成されます（詳細は[Hugging Faceのドキュメント](https://huggingface.co/docs/transformers/main/tokenizer_summary)や[OpenAIのリポジトリ](https://github.com/openai/tiktoken)をご覧ください）。\n",
    "\n",
    "ここで使用するトークナイザーがモデルと一致していることを確認してください。DatabricksのDBRX InstructモデルはGPT-4と同じトークナイザーを使用します。この例では、`transformers`ライブラリを使用して、DBRX Instructのトークナイザーでトークン数をカウントします。また、ドキュメントのトークンサイズが埋め込みの最大サイズ（1024トークン）を超えないようにします。\n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px;\">\n",
    "  <strong>情報</strong><br/>\n",
    "  以下の手順はデータセットごとに異なります。これは、成功するRAGアシスタントを構築するための重要な部分です。\n",
    "  <br/>作成されたチャンクを手動で確認し、内容が適切で関連性があることを必ず確認してください。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967e5b0b-2731-46e2-9901-30893b9c19ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "max_chunk_size = 500\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "\n",
    "def split_html_with_h3_p(html, min_chunk_size=20, max_chunk_size=500):\n",
    "    \"\"\"\n",
    "    HTMLコンテンツをh2, h3, pタグに基づいてチャンクに分割する関数。\n",
    "    分割後、チャンクがmin_chunk_size以上のトークン数であるものだけを返す。\n",
    "\n",
    "    Args:\n",
    "        html (str): 分割対象のHTML文字列\n",
    "        min_chunk_size (int): 分割後のチャンクが保持すべき最小トークン数\n",
    "        max_chunk_size (int): 各チャンクの最大トークン数\n",
    "\n",
    "    Returns:\n",
    "        list[str]: 分割されたテキストチャンクのリスト\n",
    "    \"\"\"\n",
    "\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    chunks = []\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for tag in soup.find_all([\"h2\", \"h3\", \"p\"]):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if tag.name == \"h2\":\n",
    "            # 新しいh2タグが現れた場合、既存のチャンクを分割してリストに追加。\n",
    "            if current_chunk:\n",
    "                chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "            current_h2 = text\n",
    "            current_h3 = None  # h3をリセット\n",
    "            current_chunk = f\"{current_h2}\\n\"\n",
    "\n",
    "        # elif tag.name == \"h3\":\n",
    "        #     # 新しいh3タグが現れた場合、既存のチャンクを分割してリストに追加。\n",
    "        #     if current_chunk:\n",
    "        #         chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "        #     current_h3 = text\n",
    "        #     current_chunk = f\"{current_h2}\\n{current_h3}\\n\" if current_h2 else f\"{current_h3}\\n\"\n",
    "\n",
    "        elif tag.name == \"p\":\n",
    "            # pは現在のh2またはh3に追加\n",
    "            if len(tokenizer.encode(current_chunk + text)) <= max_chunk_size:\n",
    "                current_chunk += text + \"\\n\"\n",
    "            else:\n",
    "                chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "                current_chunk = text + \"\\n\"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.extend(text_splitter.split_text(current_chunk.strip()))\n",
    "\n",
    "    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "\n",
    "# DatabricksのテーブルからHTMLデータを取得\n",
    "html = spark.table(\"raw_documentation\").limit(1).collect()[0]['text']\n",
    "split_chunks = split_html_with_h3_p(html)\n",
    "print(split_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1200980259481888,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32163cc-591d-4f00-ac55-0434fb3a48aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### チャンクの作成とDeltaテーブルへの保存\n",
    "\n",
    "最後のステップは、UDF（ユーザー定義関数）をすべてのドキュメントテキストに適用し、それらを`product_documentation`テーブルに保存することです。\n",
    "\n",
    "*この部分は通常、本番環境レベルのジョブとして設定され、新しいドキュメントページが更新され次第、実行される形になります。<br/>Delta Live Tableパイプラインとして設定し、更新を逐次的に処理することも可能です。*\n",
    "\n",
    "また、Vector Search のインデックス作成時は、対象テーブルのChange Data Feed を有効にする必要があります。  \n",
    "Change Data Feedの機能の詳細はドキュメントなどを参考にしてください。  \n",
    "  \n",
    "参考：  \n",
    "[Use Delta Lake change data feed](https://learn.microsoft.com/ja-jp/azure/databricks/delta/delta-change-data-feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3e0010-896f-4fed-845a-cd19de43c914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Change Data Feed を有効にする必要があるため、インデックス作成前に設定する\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS product_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f23007-86ea-46c5-b632-c00d5ec4adbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a user-defined function (UDF) to chunk all our documents with spark\n",
    "import pandas as pd\n",
    "\n",
    "# Spark の UDF（ユーザー定義関数）として pandas UDF を定義\n",
    "# この UDF は、各ドキュメント（HTML文字列）をチャンクに分割し、文字列の配列（array<string>）を返します\n",
    "@F.pandas_udf(\"array<string>\")\n",
    "def parse_and_split(docs: pd.Series) -> pd.Series:\n",
    "    # 各ドキュメントに対して、split_html_with_h3_p 関数を適用し、テキストチャンクに分割\n",
    "    return docs.apply(split_html_with_h3_p)\n",
    "    \n",
    "# \"raw_documentation\" テーブルからデータを取得し、UDF を用いてテキストを分割して新たなテーブル \"product_documentation\" に保存する処理\n",
    "(spark.table(\"raw_documentation\")\n",
    "      # text カラムが null でない行のみを対象とする\n",
    "      .filter('text is not null')\n",
    "      # parse_and_split UDF を適用して、text をチャンクに分割した結果を content カラムに追加\n",
    "      # F.explode により、分割された配列（各行のリスト）を個別の行に展開します\n",
    "      .withColumn('content', F.explode(parse_and_split('text')))\n",
    "      # 元の text カラムは不要なため削除\n",
    "      .drop(\"text\")\n",
    "      # 上書きモードで \"product_documentation\" テーブルとして保存\n",
    "      .write.mode('overwrite').saveAsTable(\"product_documentation\"))\n",
    "\n",
    "# 作成された \"product_documentation\" テーブルの内容を表示する\n",
    "display(spark.table(\"product_documentation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430bf5e3-22ff-4893-af74-327948bbe943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ベクター検索インデックスの要件\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-managed-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "Databricksでは、以下の複数の種類のベクター検索インデックスを提供しています：\n",
    "\n",
    "- **マネージド埋め込み (Managed embeddings)**  \n",
    "  テキストカラムとエンドポイント名を指定すると、DatabricksがDeltaテーブルとインデックスを同期します。  \n",
    "  **（今回はこれを使用します）**\n",
    "\n",
    "- **セルフマネージド埋め込み (Self Managed embeddings)**  \n",
    "  埋め込みを自分で計算し、それをDeltaテーブルのフィールドとして保存します。その後、Databricksがインデックスを同期します。\n",
    "\n",
    "- **直接インデックス (Direct index)**  \n",
    "  Deltaテーブルを使用せずにインデックスを利用・更新したい場合に使用します。\n",
    "\n",
    "ここでは、**マネージド埋め込み**インデックスの設定方法を紹介します。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cca752-5eed-4ff4-a313-35d84606647d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks GTE Embeddings Foundation Model エンドポイントの紹介\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-4.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Databricksが提供するファウンデーションモデルは、すぐに利用できる形で提供されています。\n",
    "\n",
    "Databricksでは、埋め込みを計算したりモデルを評価するために、以下のエンドポイントタイプをサポートしています：\n",
    "- **ファウンデーションモデルエンドポイント**（例: DBRX, MPT, GTE）  \n",
    "  Databricksが提供する標準エンドポイント。  \n",
    "- **外部エンドポイント**  \n",
    "  外部モデルへのゲートウェイとして機能（例: Azure OpenAI）\n",
    "  ** OpenhackではAzure OpenAIから利用できる text-embedding-ada-002 を利用します。**\n",
    "- **カスタムモデル**  \n",
    "  Databricksモデルサービスにホストされたファインチューニング済みモデル\n",
    "\n",
    "[Model Serving Endpointページ](/ml/endpoints)を開いて、ファウンデーションモデルを探索し試してみてください。\n",
    "\n",
    "このデモでは、埋め込み用にAzure OpenAIの`  ** OpenhackではAzure OpenAIから利用できる text-embedding-ada-002 を利用します。**`を、チャット用に`gpt-4o`を使用します。<br/><br/>\n",
    "\n",
    "<img src=\"/Volumes/trainer_catalog/default/src_data/images/05_serving_endpoint.png\" style=\"width: 1500px; margin-left: 10px\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945a8e7e-8494-4754-b03e-f319b3e44e75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "エンベディングの実行例"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "\n",
    "# Databricks のデプロイメントクライアントを取得\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# OpenAI の text-embedding-ada-002 を使用（エンベディング生成）を実行\n",
    "# エンドポイント \"openhack-text-embedding-ada-002\" に対して、\"What is Apache Spark?\" という入力テキストを渡す\n",
    "response = deploy_client.predict(\n",
    "    endpoint=\"openhack-text-embedding-ada-002\",\n",
    "    inputs={\"input\": [\"What is Apache Spark?\"]}\n",
    ")\n",
    "\n",
    "# 結果を取得\n",
    "embeddings = [e[\"embedding\"] for e in response.data]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617cad67-0be0-4da4-a914-ee3e33887025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Managed Embeddingsを使ったベクター検索インデックスの作成\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-3.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Managed Embeddingsを使用することで、Databricksが埋め込みを自動的に計算してくれます。これにより、Databricksの利用を簡単に始められるモードとなっています。\n",
    "\n",
    "ベクター検索インデックスは、埋め込みを提供する**ベクター検索エンドポイント**を使用します。（これをベクター検索APIエンドポイントと考えることができます。）\n",
    "\n",
    "複数のインデックスが同じエンドポイントを利用することも可能です。\n",
    "\n",
    "では、インデックスを作成してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdb79f1-63ec-4ef0-80a7-78869cbc7e20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# 以下はVector Searchエンドポイントの作成例です。\n",
    "# Vector Searchエンドポイントの作成には10分～15分ほどかかります。 本日は時間がないので、作成済みのエンドポイントを利用します。\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34b3960-c46a-4175-a6a1-abba64e3da30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\" style=\"float: right\">\n",
    "\n",
    "ベクター検索エンドポイントは[Vector Search Endpoints UI](#/setting/clusters/vector-search)で確認できます。エンドポイント名をクリックすると、そのエンドポイントで提供されるすべてのインデックスを表示できます。\n",
    "\n",
    "### ベクター検索インデックスの作成\n",
    "\n",
    "ここからは、Databricksにインデックスを作成するよう依頼するだけです。\n",
    "\n",
    "Managed Embedding Indexの場合、必要なのはテキストカラムと使用する埋め込みファウンデーションモデル（`GTE`）を指定するだけです。  \n",
    "Databricksが埋め込みの計算を自動的に行います。\n",
    "\n",
    "これはAPIを使用して行うことも、Unity Catalog Explorerメニュー内で数回のクリックで行うこともできます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70996700-0257-4538-8e3b-7c629fe48b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUIを使ったVector Search Indexの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e4d431-9985-46a7-bd75-58b0b1495b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GUIで Vector Search エンドポイントの作成を行います。\n",
    "1. 現在のノートブックの左型タブにある`Workspace (Ctrl + Alt + E)`を選択し、現在のディレクトリ（`contents`)を表示\n",
    "1. ケバブメニュー（`︙`）を選択し、`作成` -> `Genieスペース`を選択 *1\n",
    "1. Genieスペースの作成画面にて下記セルの出力結果を設定して`Save`を選択\n",
    "1. チャットウィンドウにて、`データセットに含まれるテーブルについて説明して`という質問の回答が来ることを確認\n",
    "\n",
    "*1 Genie スペースを作成できない場合には、下記の手順を実施して Genie スペースの有効化が必要です\n",
    "\n",
    "1. Databricks Workspace にて右上のユーザーアイコンを選択後、`Previews`を選択\n",
    "1. `Genie`の有効化に関するトグルを`On`に設定\n",
    "1. ページをリロード後、Databricks Workspace の左メニューにて Genie が追加されたことを確認 \n",
    "\n",
    "参考リンク\n",
    "\n",
    "- [AI/BI Genie スペースとは](https://learn.microsoft.com/ja-jp/azure/databricks/genie/)\n",
    "- [Use trusted assets in AI/BI Genie spaces](https://learn.microsoft.com/ja-jp/azure/databricks/genie/trusted-assets)\n",
    "- [効果的な Genie スペースをキュレーションする](https://learn.microsoft.com/ja-jp/azure/databricks/genie/best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da3fb9d1-6860-4ef6-81fe-567d039e7c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ベクトル検索インデックスは、GUIまたはSDKコマンドを使って作成できます。  \n",
    "ここでは、GUIを使った Delta テーブル用のベクトル検索インデックスの設定方法について説明します。\n",
    "\n",
    "まず、ソースのテーブルに移動します\n",
    "カタログメニューの、各チームのカタログ配下に、本ノートブックで作成した、05_vector_search_index_for_ユーザ名 というスキーマがあります。\n",
    "\n",
    "先ほど作成した、producct_documentationテーブルを選択肢、 画面右上の[作成] ボタンから、[ベクトル検索インデックス]を選択してください。\n",
    "\n",
    "画面例：  \n",
    "![](/Volumes/trainer_catalog/default/src_data/images/05_create_vaector_index_1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bc8386-a6cb-42ac-87e9-9dde2f520bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"/Volumes/trainer_catalog/default/src_data/images/05_create_vaector_index_2.png\" width=\"500px\" style=\"float: right\">\n",
    "\n",
    "[ベクトル検索インデックスの作成] が表示されます。\n",
    "以下の入力を行います。\n",
    "1. 名前(インデックス名)： product_documentation_vs_index と入力します\n",
    "1. プライマリキー：ソースのデルタテーブルのすべてのレコードにわたって一意の整数である必要があります。今回は id 列を指定します。\n",
    "1. エンドポイント：vs_endpoint をドロップダウンから指定します。\n",
    "1. 同期する列：空欄のままにします\n",
    "1. ソースの埋め込み：コンピュートの埋め込みを指定します\n",
    "1. 埋め込みソース列：埋め込みに変換する必要がある重要なデータを含むカラムを指定します。（複数可）。これにより、後でRAGアプローチを使用した類似性検索でクエリを実行できるようになります。  \n",
    "今回は content 列を指定します。\n",
    "1. 埋め込みモデル：Deltaテーブルのソースデータをエンベディングに変換する、Databricksが提供するエンベディングモデルを選択します。今回は、「openhack-text-embedding-ada-002」を選択します。\n",
    "1. 計算されたエンべディングを同期：Databricksは、「vector_search_index_table」を手動または自動で同期するオプションを提供しています。つまり、ソースのデルタテーブルに新しいレコードが追加されるたびに、「vector_search_index_table」が手動または自動で更新されます。ここでは2つのモードがあります。今回はトリガーを選択します。\n",
    "\n",
    "- トリガー：手動でトリガー、または外部アクションからトリガー\n",
    "- 連続：ソーステーブルに新しいレコードが追加されるたびに自動でトリガー\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8719a0f-b218-4c83-b506-3be2b334fca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 参考 Vector Search Indexの作成をコードで実行する例です。\n",
    "# GUIで設定済みのため、実行不要です。Index作成に少し時間がかかるので、参考にご確認ください。\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.product_documentation\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.product_documentation_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  try:\n",
    "    vsc.create_delta_sync_index(\n",
    "      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "      index_name=vs_index_fullname,\n",
    "      source_table_name=source_table_fullname,\n",
    "      pipeline_type=\"TRIGGERED\",\n",
    "      primary_key=\"id\",\n",
    "      embedding_source_column='content', #The column containing our text\n",
    "      # embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "      embedding_model_endpoint_name='openhack-text-embedding-ada-002'\n",
    "    )\n",
    "  except Exception as e:\n",
    "    display_quota_error(e, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "    raise e\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bb5a86-da5e-4326-80e7-665955521287",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "今回取り込んだ html の例(製品FAQのページ)"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Volumes/trainer_catalog/default/src_data/openhack-llm/faq.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "  document_html = f.read()\n",
    "\n",
    "displayHTML(document_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe930b5-537c-429b-9ac6-1a56a639cf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 類似コンテンツの検索\n",
    "\n",
    "これで準備は完了です。DatabricksはDelta Live Table内の新しいエントリを自動的にキャプチャし、同期します。\n",
    "\n",
    "データセットのサイズやモデルの規模に応じて、インデックス作成が始まり、埋め込みをインデックス化するまで数秒かかる場合があります。\n",
    "\n",
    "では、実際に試して類似コンテンツを検索してみましょう。\n",
    "\n",
    "*補足: `similarity_search` ではフィルターパラメータもサポートされています。これはRAGシステムにセキュリティレイヤーを追加するのに便利です。例えば、ユーザーの所属部門に基づいて特定のデータをフィルタリングすることで、機密性の\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f02fcf-d729-432a-8a7a-f0a7dec0073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "vs_index_fullname = f\"{catalog}.{db}.product_documentation_vs_index\"\n",
    "\n",
    "question = \"バッテリー交換\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1200980259481920,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "05_vector_search_index",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
