{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6a893fd-3e54-4438-924c-6c2652ee7c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96b92938-a5bd-41fd-b55a-90a14ae9a77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"reset_all_data\", \"false\", \"Reset Data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf79449-737e-4b14-9495-c66909a3b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# When running in a job, writting to the local file fails. This simply skip it as we ship the file in the repo so there is no need to write it if it's running from the repo\n",
    "# Note: we don't ship the chain file when we install it with dbdemos.instal(...).\n",
    "@register_cell_magic\n",
    "def writefile(line, cell):\n",
    "    filename = line.strip()\n",
    "    try:\n",
    "      folder_path = os.path.dirname(filename)\n",
    "      if len(folder_path) > 0:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "      with open(filename, 'w') as f:\n",
    "          f.write(cell)\n",
    "          print('file overwritten')\n",
    "    except Exception as e:\n",
    "      print(f\"WARN: could not write the file {filename}. If it's running as a job it's to be expected, otherwise something is off - please print the message for more details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f692c36-0f2e-4429-9396-f653cb6d4224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf\n",
    "import os\n",
    "import mlflow\n",
    "import yaml\n",
    "from typing import Iterator\n",
    "from mlflow import MlflowClient\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# Workaround for a bug fix that is in progress\n",
    "mlflow.spark.autolog(disable=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Disable MLflow warnings\n",
    "import logging\n",
    "logging.getLogger('mlflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f85570e1-5425-4481-9c3b-7012b8e0795b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if reset_all_data:\n",
    "  print(f'clearing up db {dbName}')\n",
    "  spark.sql(f\"DROP DATABASE IF EXISTS `{dbName}` CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7034f671-d36b-41a4-a7d1-9e2cd5f90456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "assert catalog not in ['hive_metastore', 'spark_catalog'], \"Please use a UC schema\"\n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs:\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "      if catalog == 'dbdemos':\n",
    "        spark.sql(f\"ALTER CATALOG {catalog} OWNER TO `account users`\")\n",
    "  use_and_create_db(catalog, dbName)\n",
    "\n",
    "\n",
    "print(f\"using catalog.database `{catalog}`.`{dbName}`\")\n",
    "spark.sql(f\"\"\"USE `{catalog}`.`{dbName}`\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462909a4-e7f9-4dac-805c-3a2a4e3f29b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"databricks_documentation\") or spark.table(\"databricks_documentation\").isEmpty() or \\\n",
    "    not spark.catalog.tableExists(\"eval_set_databricks_documentation\") or spark.table(\"eval_set_databricks_documentation\").isEmpty():\n",
    "  spark.sql('''CREATE TABLE IF NOT EXISTS databricks_documentation (\n",
    "            id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "            url STRING,\n",
    "            content STRING\n",
    "          ) TBLPROPERTIES (delta.enableChangeDataFeed = true)''')\n",
    "  (spark.createDataFrame(pd.read_parquet('https://notebooks.databricks.com/demos/dbdemos-dataset/llm/databricks-documentation/databricks_documentation.parquet'))\n",
    "   .drop('title').write.mode('overwrite').saveAsTable(\"databricks_documentation\"))\n",
    "  (spark.createDataFrame(pd.read_parquet('https://notebooks.databricks.com/demos/dbdemos-dataset/llm/databricks-documentation/databricks_doc_eval_set.parquet'))\n",
    "   .write.mode('overwrite').saveAsTable(\"eval_set_databricks_documentation\"))\n",
    "  #Make sure enableChangeDataFeed is enabled\n",
    "  spark.sql('ALTER TABLE databricks_documentation SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\n",
    "\n",
    "\n",
    "def display_txt_as_html(txt):\n",
    "    txt = txt.replace('\\n', '<br/>')\n",
    "    displayHTML(f'<div style=\"max-height: 150px\">{txt}</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291a19dd-bea1-4811-b516-e5409d31a79a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optional: Allowing Model Serving IPs"
    }
   },
   "outputs": [],
   "source": [
    "#If your workspace has ip access list, you need to allow your model serving endpoint to hit your AI gateway. Based on your region, IPs might change. Please reach out your Databrics Account team for more details.\n",
    "\n",
    "#def allow_serverless_ip():\n",
    "#  from databricks.sdk import WorkspaceClient\n",
    "#  from databricks.sdk.service import settings\n",
    "#\n",
    "#  w = WorkspaceClient()\n",
    "#\n",
    "#  # cleanup\n",
    "#  w.ip_access_lists.delete(ip_access_list_id='xxxx')\n",
    "#  created = w.ip_access_lists.create(label=f'serverless-model-serving',\n",
    "#                                    ip_addresses=['xxxx/32'],\n",
    "#                                    list_type=settings.ListType.ALLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f432195-ebf5-4d79-a93e-294aa510e047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_latest_model_version(model_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d8b385-778d-4ca4-94af-55dbb52f5153",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deaf4197-ebbe-4cf5-8377-c23e70569e8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "index"
    }
   },
   "outputs": [],
   "source": [
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "            raise e\n",
    "    return False\n",
    "    \n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")\n",
    "\n",
    "def wait_for_model_serving_endpoint_to_be_ready(ep_name):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "    import time\n",
    "\n",
    "    # TODO make the endpoint name as a param\n",
    "    # Wait for it to be ready\n",
    "    w = WorkspaceClient()\n",
    "    state = \"\"\n",
    "    for i in range(200):\n",
    "        state = w.serving_endpoints.get(ep_name).state\n",
    "        if state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "            if i % 40 == 0:\n",
    "                print(f\"Waiting for endpoint to deploy {ep_name}. Current state: {state}\")\n",
    "            time.sleep(10)\n",
    "        elif state.ready == EndpointStateReady.READY:\n",
    "          print('endpoint ready.')\n",
    "          return\n",
    "        else:\n",
    "          break\n",
    "    raise Exception(f\"Couldn't start the endpoint, timeout, please check your endpoint for more details: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f03a7361-4a10-4779-a097-2bdee88342c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StringType\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "#Add retries with backoff to avoid 429 while fetching the doc\n",
    "retries = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=3,\n",
    "    status_forcelist=[429],\n",
    ")\n",
    "\n",
    "def download_databricks_documentation_articles(max_documents=None):\n",
    "    # Fetch the XML content from sitemap\n",
    "    response = requests.get(DATABRICKS_SITEMAP_URL)\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    # Find all 'loc' elements (URLs) in the XML\n",
    "    urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
    "    if max_documents:\n",
    "        urls = urls[:max_documents]\n",
    "\n",
    "    # Create DataFrame from URLs\n",
    "    df_urls = spark.createDataFrame(urls, StringType()).toDF(\"url\").repartition(10)\n",
    "\n",
    "    # Pandas UDF to fetch HTML content for a batch of URLs\n",
    "    @pandas_udf(\"string\")\n",
    "    def fetch_html_udf(urls: pd.Series) -> pd.Series:\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        http = requests.Session()\n",
    "        http.mount(\"http://\", adapter)\n",
    "        http.mount(\"https://\", adapter)\n",
    "        def fetch_html(url):\n",
    "            try:\n",
    "                response = http.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.content\n",
    "            except requests.RequestException:\n",
    "                return None\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "            results = list(executor.map(fetch_html, urls))\n",
    "        return pd.Series(results)\n",
    "\n",
    "    # Pandas UDF to process HTML content and extract text\n",
    "    @pandas_udf(\"string\")\n",
    "    def download_web_page_udf(html_contents: pd.Series) -> pd.Series:\n",
    "        def extract_text(html_content):\n",
    "            if html_content:\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                article_div = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "                if article_div:\n",
    "                    return str(article_div).strip()\n",
    "            return None\n",
    "\n",
    "        return html_contents.apply(extract_text)\n",
    "\n",
    "    # Apply UDFs to DataFrame\n",
    "    df_with_html = df_urls.withColumn(\"html_content\", fetch_html_udf(\"url\"))\n",
    "    final_df = df_with_html.withColumn(\"text\", download_web_page_udf(\"html_content\"))\n",
    "\n",
    "    # Select and filter non-null results\n",
    "    final_df = final_df.select(\"url\", \"text\").filter(\"text IS NOT NULL\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "#doc = download_databricks_documentation_articles(100)\n",
    "#doc.write.mode('overwrite').saveAsTable('databricks_documentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7dd011d-8f42-42f3-8458-fba7b8e8982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_gradio_app(space_name = \"databricks-demos-chatbot\"):\n",
    "    displayHTML(f'''<div style=\"margin: auto; width: 1000px\"><iframe src=\"https://{space_name}.hf.space\" frameborder=\"0\" width=\"1000\" height=\"950\" style=\"margin: auto\"></iframe></div>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58524f8c-8e99-4e45-9375-1abb4a81c5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display a better quota message \n",
    "def display_quota_error(e, ep_name):\n",
    "  if \"QUOTA_EXCEEDED\" in str(e): \n",
    "    displayHTML(f'<div style=\"background-color: #ffd5b8; border-radius: 15px; padding: 20px;\"><h1>Error: Vector search Quota exceeded in endpoint {ep_name}</h1><p>Please select another endpoint in the ../config file (VECTOR_SEARCH_ENDPOINT_NAME=\"<your-endpoint-name>\"), or <a href=\"/compute/vector-search\" target=\"_blank\">open the vector search compute page</a> to cleanup resources.</p></div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7166b3-b6ab-4e07-a3c1-a691e5a0e626",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleanup utility to remove demo assets"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_demo(catalog, db, serving_endpoint_name, vs_index_fullname):\n",
    "  vsc = VectorSearchClient()\n",
    "  try:\n",
    "    vsc.delete_index(endpoint_name = VECTOR_SEARCH_ENDPOINT_NAME, index_name=vs_index_fullname)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete index {VECTOR_SEARCH_ENDPOINT_NAME} {vs_index_fullname} - might not be existing: {e}\")\n",
    "  try:\n",
    "    WorkspaceClient().serving_endpoints.delete(serving_endpoint_name)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete serving endpoint {serving_endpoint_name} - might not be existing: {e}\")\n",
    "  spark.sql(f'DROP SCHEMA `{catalog}`.`{db}` CASCADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bafe6fe0-e48c-4324-8c6b-55a5b156c974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pprint(obj):\n",
    "  import pprint\n",
    "  pprint.pprint(obj, compact=True, indent=1, width=100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-init",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
