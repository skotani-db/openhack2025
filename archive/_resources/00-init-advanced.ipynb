{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbccae86-b6bd-4a25-a6b1-efff6306ceee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# init notebook setting up the backend. \n",
    "\n",
    "Do not edit the notebook, it contains import and helpers for the demo\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=2952210246698934&notebook=%2F_resources%2F00-init-advanced&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F_resources%2F00-init-advanced&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a11a964f-b954-42f0-b8d2-13093e6eaba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fadb321d-5955-422b-8aa6-252f21d4ae29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    " \n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "  \n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "          r.raise_for_status()\n",
    "          print('saving '+destination+'/'+local_filename)\n",
    "          with open(destination+'/'+local_filename, 'wb') as f:\n",
    "              for chunk in r.iter_content(chunk_size=8192): \n",
    "                  # If you have chunk encoded response uncomment if\n",
    "                  # and set chunk_size parameter to None.\n",
    "                  #if chunk: \n",
    "                  f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    files = [f.replace(\n",
    "            \"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\",\n",
    "            \"https://dbdemos-dataset.s3.amazonaws.com/\"\n",
    "        ) for f in files]\n",
    "    def download_to_dest(url):\n",
    "         download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d7e6cb-f536-4e6e-b107-0fe63a234033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_pdfs_to_volume(volume_path):\n",
    "  download_file_from_git(volume_path, \"databricks-demos\", \"dbdemos-dataset\", \"/llm/databricks-pdf-documentation\")\n",
    "\n",
    "def upload_dataset_to_volume(volume_path):\n",
    "  download_file_from_git(volume_path, \"databricks-demos\", \"dbdemos-dataset\", \"/llm/databricks-documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4048e5b-7020-42eb-b18d-6149807a59d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deduplicate_assessments_table(assessment_table):\n",
    "    # De-dup response assessments\n",
    "    assessments_request_deduplicated_df = spark.sql(f\"\"\"select * except(row_number)\n",
    "                                        from ( select *, row_number() over (\n",
    "                                                partition by request_id\n",
    "                                                order by\n",
    "                                                timestamp desc\n",
    "                                            ) as row_number from {assessment_table} where text_assessment is not NULL\n",
    "                                        ) where row_number = 1\"\"\")\n",
    "    # De-dup the retrieval assessments\n",
    "    assessments_retrieval_deduplicated_df = spark.sql(f\"\"\"select * except( retrieval_assessment, source, timestamp, text_assessment, schema_version),\n",
    "        any_value(timestamp) as timestamp,\n",
    "        any_value(source) as source,\n",
    "        collect_list(retrieval_assessment) as retrieval_assessments\n",
    "      from {assessment_table} where retrieval_assessment is not NULL group by request_id, source.id, step_id\"\"\"    )\n",
    "\n",
    "    # Merge together\n",
    "    assessments_request_deduplicated_df = assessments_request_deduplicated_df.drop(\"retrieval_assessment\", \"step_id\")\n",
    "    assessments_retrieval_deduplicated_df = assessments_retrieval_deduplicated_df.withColumnRenamed(\"request_id\", \"request_id2\").withColumnRenamed(\"source\", \"source2\").drop(\"step_id\", \"timestamp\")\n",
    "\n",
    "    merged_deduplicated_assessments_df = assessments_request_deduplicated_df.join(\n",
    "        assessments_retrieval_deduplicated_df,\n",
    "        (assessments_request_deduplicated_df.request_id == assessments_retrieval_deduplicated_df.request_id2) &\n",
    "        (assessments_request_deduplicated_df.source.id == assessments_retrieval_deduplicated_df.source2.id),\n",
    "        \"full\"\n",
    "    ).select(\n",
    "        [str(col) for col in assessments_request_deduplicated_df.columns] +\n",
    "        [assessments_retrieval_deduplicated_df.retrieval_assessments]\n",
    "    )\n",
    "\n",
    "    return merged_deduplicated_assessments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7dd0fdf-98c8-446d-821d-4b0444e92d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_latest_model(model_name):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = None\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if not latest_version or version_int > int(latest_version.version):\n",
    "            latest_version = mv\n",
    "    return latest_version"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-init-advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
